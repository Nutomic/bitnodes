[general]

# Name of this cryptocurrency
coin_name = dash

# Network magic number
magic_number = bf0c6bbd

# Redis database number
db = 2

# Default/fallback port number
port = 9999

# User agent (BIP 0014) to use for outgoing version message
# -----------------------------------------------------------------------------
#                                 NOTE TO USERS
# Please consider changing the user agent before running an instance of this
# crawler. This is so that users will not confuse your crawler with another
# instance that is already running and generating data for the project.
# -----------------------------------------------------------------------------
user_agent = /coincrawler:0.1/

# Services to use for outgoing network address message
# https://github.com/dashpay/dash/blob/master/src/protocol.h#L277
services = 0

# Attempt to establish connection with .onion nodes
onion = True

# Tor proxy is required to connect to .onion address
tor_proxy = 127.0.0.1:9050

# Protocol version to use for outgoing version message
protocol_version = 70210

# Minimum protocol version of crawled nodes
min_protocol_version = 70208

# Persistent storage for historical crawl results
storage_file = data/storage/dash.sqlite

[crawl]

# Logfile
logfile = log/dash/crawl.log

# List of DNS seeders to get a subset of reachable nodes
seeders =
    dnsseed.dash.org
    dnsseed.dashdot.io
    dnsseed.masternode.io
    dnsseed.dashpay.io

# Number of concurrent workers (greenlets)
workers = 350

# Print debug output
debug = False

# Public IP address for network interface
source_address = 0.0.0.0

# Set to 1 to receive all txs
relay = 0

# Socket timeout
socket_timeout = 30

# Run cron tasks every given interval
cron_delay = 10

# Take full network snapshot at most at every given interval
snapshot_delay = 240

# Max. age for peering node to be included in crawl set
max_age = 86400

# Attempt to establish connection with IPv6 nodes
ipv6 = True

# Limit max. nodes per IPv6 network prefix
ipv6_prefix = 64
nodes_per_ipv6_prefix = 1

# List of excluded ASNs
exclude_asns =

# List of excluded IPv4 networks
exclude_ipv4_networks =
    0.0.0.0/8
    10.0.0.0/8
    100.64.0.0/10
    127.0.0.0/8
    169.254.0.0/16
    172.16.0.0/12
    192.0.0.0/24
    192.0.0.0/29
    192.0.0.170/32
    192.0.0.171/32
    192.0.0.8/32
    192.0.2.0/24
    192.168.0.0/16
    192.175.48.0/24
    192.31.196.0/24
    192.52.193.0/24
    192.88.99.0/24
    198.18.0.0/15
    198.51.100.0/24
    203.0.113.0/24
    240.0.0.0/4
    255.255.255.255/32

# List of excluded IPv6 networks
exclude_ipv6_networks =

# Exclude IPv4 bogons
exclude_ipv4_bogons = True

# List of initial .onion nodes
onion_nodes =
    darkcoinie7ghp67.onion
    drktalkwaybgxnoq.onion
    drkcoinooditvool.onion
    darkcoxbtzggpmcc.onion
    ssapp53tmftyjmjb.onion
    j2dfl3cwxyxpbc7s.onion
    vf6d2mxpuhh2cbxt.onion
    rj24sicr6i4vsnkv.onion
    wrwx2dy7jyh32o53.onion
    f5ekot4ajkbe23gt.onion
    dshtord4mqvgzqev.onion

# Relative path to directory containing timestamp-prefixed JSON crawl files
crawl_dir = data/crawl/dash

[export]

# Logfile
logfile = log/dash/export.log

# Print debug output
debug = False

# Maximum number of that a node's block height may differ from the median block
# to be considered synced
max_block_height_difference = 12

# Include blocks with more than max_block_height_difference in export
include_out_of_sync = True

# Relative path to directory containing timestamp-prefixed JSON export files
export_dir = data/export/dash

# URL to a public instance of the Dash insight API
dash_insight_api = https://insight.dash.org/api/

[ping]

# Logfile
logfile = log/dash/ping.log

# Max. number of concurrent workers (greenlets) in a pool
workers = 1000

# Print debug output
debug = False

# Public IP address for network interface
source_address = 0.0.0.0

# Set to 1 to receive all txs
relay = 0

# Socket timeout
socket_timeout = 30

# Run cron tasks every given interval
cron_delay = 10

# Limit max. nodes per IPv6 network prefix
ipv6_prefix = 64
nodes_per_ipv6_prefix = 1

# Redis TTL for cached RTT
ttl = 10800

# Relative path to directory containing timestamp-prefixed JSON crawl files
crawl_dir = data/crawl/dash

[resolve]

# Logfile
logfile = log/dash/resolve.log

# Print debug output
debug = False

# Redis TTL for cached hostname and GeoIP data
ttl = 86400
